## PremAI

[PremAI](https://app.premai.io)  is an all-in-one platform that simplifies the process of creating robust, production-ready applications powered by Generative AI. By streamlining the development process, PremAI allows you to concentrate on enhancing user experience and driving overall growth for your application.

### Prerequisites

Refer to the [quick start](https://docs.premai.io/introduction) guide to getting started with the PremAI platform, create your first project and grab your API key.

### Setting up the PremAI Client

The constructor initializes the base class `LM` to support prompting requests to supported PremAI hosted models. This requires the following parameters:

- `model` (_str_): Models supported by PremAI. Example: `mistral-tiny`. We recommend using the model selected in [project launchpad](https://docs.premai.io/get-started/launchpad).
- `project_id` (_int_): The [project id](https://docs.premai.io/get-started/projects) which contains the model of choice.
- `api_key` (_Optional[str]_, _optional_): API provider from PremAI. Defaults to None.
- `**kwargs`: Additional language model arguments will be passed to the API provider.

Example of PremAI constructor:

```python
class PremAI(LM):
    def __init__(
        self,
        model: str,
        project_id: int,
        api_key: str,
        base_url: Optional[str] = None,
        session_id: Optional[int] = None,
        **kwargs,
    ) -> None:
```

### Under the Hood

#### `__call__(self, prompt: str, **kwargs) -> str`

**Parameters:**
- `prompt` (_str_): Prompt to send to PremAI.
- `**kwargs`: Additional keyword arguments for completion request. You can find all the additional kwargs [here](https://docs.premai.io/get-started/sdk#optional-parameters).

**Returns:**
- `str`: Completions string from the chosen LLM provider 

Internally, the method handles the specifics of preparing the request prompt and corresponding payload to obtain the response.

### Using the PremAI client

```python
premai_client = dspy.PremAI(project_id=1111)
```

Please note that, this is a dummy `project_id`. You need to change this to the project_id you are interested to use with dspy. 

```python
dspy.configure(lm=premai_client)

#Example DSPy CoT QA program
qa = dspy.ChainOfThought('question -> answer')

response = qa(question="What is the capital of Paris?") 
print(response.answer)
```

2) Generate responses using the client directly.

```python
response = premai_client(prompt='What is the capital of Paris?')
print(response)
```

### Native RAG Support

Prem Repositories which allows users to upload documents (.txt, .pdf etc) and connect those repositories to the LLMs. You can think Prem repositories as native RAG, where each repository can be considered as a vector database. You can connect multiple repositories. You can learn more about repositories [here](https://docs.premai.io/get-started/repositories).

Repositories are also supported in dspy premai. Here is how you can do it. 

```python
query = "what is the diameter of individual Galaxy"
repository_ids = [1991, ]
repositories = dict(
    ids=repository_ids,
    similarity_threshold=0.3,
    limit=3
)
```

First we start by defining our repository with some repository ids. Make sure that the ids are valid repository ids. You can learn more about how to get the repository id [here](https://docs.premai.io/get-started/repositories). 

> Please note: Similar like `model` when you invoke the argument `repositories`, then you are potentially overriding the repositories connected in the launchpad. 

Now, we connect the repository with our chat object to invoke RAG based generations. 

```python 
response = premai_client(prompt=query, max_tokens=100, repositories=repositories)

print(response)
print("---")
print(json.dumps(premai_client.history, indent=4))
```