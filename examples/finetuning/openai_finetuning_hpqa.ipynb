{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Fine-tuning Demo with HotPotQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning demo with DSPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic commands and secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "assert \"DSP_CACHEDIR\" in os.environ\n",
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "\n",
    "# Altenatively, you can set the environment variables in code\n",
    "# os.environ[\"DSP_CACHEDIR\"] = <YOUR_CACHE_DIR>\n",
    "# os.environ[\"OPENAI_API_KEY\"] = <OPENAI_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import dspy.evaluate\n",
    "from dspy.datasets import HotPotQA\n",
    "from dspy.evaluate import Evaluate\n",
    "from dsp.utils.utils import deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = \"gpt-4o-mini-2024-07-18\"\n",
    "base_temp = 0.9\n",
    "\n",
    "lm = dspy.TrainableOpenAI(model=mini, max_tokens=500, temperature=base_temp)\n",
    "\n",
    "colbert_v2_endpoint = \"http://20.102.90.50:2017/wiki17_abstracts\"\n",
    "colbertv2 = dspy.ColBERTv2(url=colbert_v2_endpoint)\n",
    "\n",
    "dspy.settings.configure(rm=colbertv2, lm=lm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dspy.evaluate.answer_exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and configure the datasets.\n",
    "TRAIN_SIZE = 500\n",
    "EVAL_SIZE = 500\n",
    "\n",
    "hotpot_dataset = HotPotQA(train_seed=1, eval_seed=2023, test_size=0, keep_details=\"type\")\n",
    "trainset = [x.with_inputs('question') for x in hotpot_dataset.train][:EVAL_SIZE]\n",
    "devset = [x.with_inputs('question') for x in hotpot_dataset.dev][:EVAL_SIZE]\n",
    "\n",
    "# Set up metrics\n",
    "NUM_THREADS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(num_threads=NUM_THREADS, display_progress=True)\n",
    "evaluate = Evaluate(devset=devset, metric=metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMH(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(2)]\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n",
    "\n",
    "    def forward(self, question, return_trace=False):\n",
    "        context = []\n",
    "        for hop in range(2):\n",
    "            search_query = self.generate_query[hop](context=context, question=question).search_query\n",
    "            passages = self.retrieve(search_query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        x = self.generate_answer(context=context, question=question).copy(context=context)\n",
    "        \n",
    "        if return_trace:\n",
    "            return x, dspy.settings.trace\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt.random_search import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "program_params = {\n",
    "    \"passages_per_hop\": 3,\n",
    "}\n",
    "\n",
    "COMPILE = False\n",
    "\n",
    "if COMPILE:\n",
    "    max_bootstrapped_demos, max_labeled_demos, num_candidate_programs = 3,3,6\n",
    "    config = dict(max_bootstrapped_demos=max_bootstrapped_demos, num_candidate_programs=num_candidate_programs, num_threads=NUM_THREADS)\n",
    "    teleprompter = BootstrapFewShotWithRandomSearch(metric=metric, **config)\n",
    "    basicmh_bs = teleprompter.compile(BasicMH(**program_params), trainset=trainset[:100], valset=devset[:150])\n",
    "    basicmh_bs.save(f\"basicmh_{max_bootstrapped_demos}_{max_labeled_demos}_{num_candidate_programs}.json\")\n",
    "\n",
    "    baseline_eval = evaluate(BasicMH(**program_params), devset=devset[:300])\n",
    "    bs_eval = evaluate(basicmh_bs, devset=devset[:300])\n",
    "else:\n",
    "    basicmh_bs = BasicMH(**program_params)\n",
    "    basicmh_bs.load(\"basicmh_3_3_6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 113 / 200  (56.5): 100%|██████████| 200/200 [00:00<00:00, 296.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset with length 10 to trainset_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30 / 50  (60.0): 100%|██████████| 50/50 [00:00<00:00, 308.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset with length 10 to devset_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt.finetune_teleprompter import bootstrap_data_for_round, convert_to_prompt_completion_data\n",
    "import ujson\n",
    "\n",
    "samples = 200\n",
    "\n",
    "dspy.settings.configure(experimental=True)\n",
    "dc_kwargs = {\n",
    "    \"sampling_temperature\": base_temp,\n",
    "    \"sampling_temperature_delta\":0.0001,\n",
    "    \"num_threads\": NUM_THREADS,\n",
    "}\n",
    "\n",
    "dataset_filenames = {\"trainset_data.jsonl\": trainset[:samples], \"devset_data.jsonl\": devset[:int(samples/4)]}\n",
    "\n",
    "def write_data(data, filename):\n",
    "    # get the bootstrapped data for num_rounds=1, but using the callback\n",
    "    data = bootstrap_data_for_round(basicmh_bs, data, metric, sampling_round=1, **dc_kwargs)\n",
    "\n",
    "    # Post process the data to remove any entries with no score\n",
    "    filtered_data = [d for d in data if d[\"score\"]]\n",
    "\n",
    "    # Convert the data to prompt completion format\n",
    "    dataset = convert_to_prompt_completion_data(filtered_data, program=basicmh_bs, exclude_demos=True)[:10]\n",
    "    \n",
    "    # Format the data for finetuning using the LM\n",
    "    print(\"Writing dataset with length\", len(dataset), \"to\", filename)\n",
    "    with open(filename, \"w\") as f:\n",
    "        ujson.dump(dataset, f)\n",
    "\n",
    "for key, data in dataset_filenames.items():\n",
    "    write_data(data, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n",
      "\n",
      "10 examples are missing a system message\n",
      "Dataset has ~4375 tokens that will be charged for during training\n",
      "By default, you'll train for 10 epochs on this dataset\n",
      "By default, you'll be charged for ~43750 tokens\n",
      "No errors found\n",
      "Uploaded train data to OpenAI\n",
      "Uploaded val data to OpenAI\n",
      "Waiting for training to complete\n"
     ]
    }
   ],
   "source": [
    "from dsp.modules.lm import TrainingMethod\n",
    "\n",
    "future_lm = lm.get_finetune(method=TrainingMethod.SFT, train_path=\"trainset_data.jsonl\", eval_path=\"devset_data.jsonl\", hyperparameters={\"n_epochs\": 1})\n",
    "finetuned_lm = future_lm.result()\n",
    "finetuned_lm.kwargs[\"temperature\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert finetuned_lm.kwargs[\"model\"] != lm.kwargs[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicmh_bs_ft = BasicMH(**program_params)\n",
    "basicmh_bs_ft._set_all_predictor_lms(finetuned_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 3 traces per predictor.\n",
      "Will attempt to bootstrap 6 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 103 / 250  (41.2): 100%|██████████| 250/250 [01:44<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 41.2 for set: [0, 0, 0]\n",
      "New best score: 41.2 for seed -3\n",
      "Scores so far: [41.2]\n",
      "Best score: 41.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 135 / 250  (54.0): 100%|██████████| 250/250 [00:32<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 54.0 for set: [16, 16, 16]\n",
      "New best score: 54.0 for seed -2\n",
      "Scores so far: [41.2, 54.0]\n",
      "Best score: 54.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:22<07:06,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 144 / 250  (57.6): 100%|██████████| 250/250 [01:37<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 57.6 for set: [16, 16, 16]\n",
      "New best score: 57.6 for seed -1\n",
      "Scores so far: [41.2, 54.0, 57.6]\n",
      "Best score: 57.6\n",
      "Average of max per entry across top 1 scores: 0.576\n",
      "Average of max per entry across top 2 scores: 0.628\n",
      "Average of max per entry across top 3 scores: 0.64\n",
      "Average of max per entry across top 5 scores: 0.64\n",
      "Average of max per entry across top 8 scores: 0.64\n",
      "Average of max per entry across top 9999 scores: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:14<08:02,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 140 / 250  (56.0): 100%|██████████| 250/250 [01:50<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 56.0 for set: [16, 16, 16]\n",
      "Scores so far: [41.2, 54.0, 57.6, 56.0]\n",
      "Best score: 57.6\n",
      "Average of max per entry across top 1 scores: 0.576\n",
      "Average of max per entry across top 2 scores: 0.64\n",
      "Average of max per entry across top 3 scores: 0.664\n",
      "Average of max per entry across top 5 scores: 0.672\n",
      "Average of max per entry across top 8 scores: 0.672\n",
      "Average of max per entry across top 9999 scores: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:12<06:36,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 137 / 250  (54.8): 100%|██████████| 250/250 [01:39<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 54.8 for set: [16, 16, 16]\n",
      "Scores so far: [41.2, 54.0, 57.6, 56.0, 54.8]\n",
      "Best score: 57.6\n",
      "Average of max per entry across top 1 scores: 0.576\n",
      "Average of max per entry across top 2 scores: 0.64\n",
      "Average of max per entry across top 3 scores: 0.668\n",
      "Average of max per entry across top 5 scores: 0.684\n",
      "Average of max per entry across top 8 scores: 0.684\n",
      "Average of max per entry across top 9999 scores: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:05<04:15,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 73 / 139  (52.5):  55%|█████▌    | 138/250 [00:54<00:29,  3.82it/s]WARNING:dspy.evaluate.evaluate:\u001b[2m2024-08-15T01:50:04.428079Z\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mReceived SIGINT. Cancelling evaluation.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m91\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(max_bootstrapped_demos\u001b[38;5;241m=\u001b[39mmax_bootstrapped_demos, num_candidate_programs\u001b[38;5;241m=\u001b[39mnum_candidate_programs, num_threads\u001b[38;5;241m=\u001b[39mNUM_THREADS)\n\u001b[1;32m      6\u001b[0m bsfsrs_teleprompter \u001b[38;5;241m=\u001b[39m BootstrapFewShotWithRandomSearch(metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m----> 7\u001b[0m basicmh_bs_ft_bs \u001b[38;5;241m=\u001b[39m \u001b[43mbsfsrs_teleprompter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasicmh_bs_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m basicmh_bs_ft_bs\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini_bs_ft_bs_hpqa_100.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m basicmh_bs_ft_bs\u001b[38;5;241m.\u001b[39m_set_all_predictor_lms(finetuned_lm)\n",
      "File \u001b[0;32m~/projects/dspy-finetuning/dilara-dspy/dspy/teleprompt/random_search.py:123\u001b[0m, in \u001b[0;36mBootstrapFewShotWithRandomSearch.compile\u001b[0;34m(self, student, teacher, trainset, valset, restrict, labeled_sample)\u001b[0m\n\u001b[1;32m    112\u001b[0m     program2 \u001b[38;5;241m=\u001b[39m teleprompter\u001b[38;5;241m.\u001b[39mcompile(student, teacher\u001b[38;5;241m=\u001b[39mteacher, trainset\u001b[38;5;241m=\u001b[39mtrainset2)\n\u001b[1;32m    114\u001b[0m evaluate \u001b[38;5;241m=\u001b[39m Evaluate(\n\u001b[1;32m    115\u001b[0m     devset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalset,\n\u001b[1;32m    116\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 123\u001b[0m score, subscores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m all_subscores\u001b[38;5;241m.\u001b[39mappend(subscores)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m############ Assertion-aware Optimization ############\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/dspy-finetuning/dilara-dspy/dspy/evaluate/evaluate.py:193\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_single_thread(wrapped_program, devset, display_progress)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_multi_thread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mncorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mntotal,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m predicted_devset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(reordered_devset)\n",
      "File \u001b[0;32m~/projects/dspy-finetuning/dilara-dspy/dspy/evaluate/evaluate.py:109\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread\u001b[0;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[1;32m    106\u001b[0m futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(cancellable_wrapped_program, idx, arg) \u001b[38;5;28;01mfor\u001b[39;00m idx, arg \u001b[38;5;129;01min\u001b[39;00m devset}\n\u001b[1;32m    107\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(devset), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m display_progress)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m    110\u001b[0m     example_idx, example, prediction, score \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# use the cancelled_job literal to check if the job was cancelled - use \"is\" not \"==\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# in case the prediction is \"cancelled\" for some reason.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/dspy-finetuning/dilara-dspy/dspy/evaluate/evaluate.py:92\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread.<locals>.interrupt_handler_manager.<locals>.interrupt_handler\u001b[0;34m(sig, frame)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_jobs\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m     91\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived SIGINT. Cancelling evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[43mdefault_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RECOMPILE_FT_MODEL = True\n",
    "\n",
    "if RECOMPILE_FT_MODEL:\n",
    "    max_bootstrapped_demos, max_labeled_demos, num_candidate_programs = 3,3,6\n",
    "    config = dict(max_bootstrapped_demos=max_bootstrapped_demos, num_candidate_programs=num_candidate_programs, num_threads=NUM_THREADS)\n",
    "    bsfsrs_teleprompter = BootstrapFewShotWithRandomSearch(metric=metric, **config)\n",
    "    basicmh_bs_ft_bs = bsfsrs_teleprompter.compile(student=basicmh_bs_ft, trainset=trainset[:100], valset=devset[:250])\n",
    "    basicmh_bs_ft_bs.save('mini_bs_ft_bs_hpqa_100.json')\n",
    "    basicmh_bs_ft_bs._set_all_predictor_lms(finetuned_lm)\n",
    "else:\n",
    "    basicmh_bs_ft_bs = BasicMH(**program_params)\n",
    "    basicmh_bs_ft_bs.load('mini_bs_ft_bs_hpqa_100.json')\n",
    "    basicmh_bs_ft_bs._set_all_predictor_lms(finetuned_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 300\n",
    "baseline_eval = evaluate(BasicMH(**program_params), devset=devset[:TEST_SIZE])\n",
    "bs_eval = evaluate(basicmh_bs, devset=devset[:TEST_SIZE])\n",
    "bs_ft_eval = evaluate(basicmh_bs_ft, devset=devset[:TEST_SIZE])\n",
    "bs_ft_bs_eval = evaluate(basicmh_bs_ft_bs, devset=devset[:TEST_SIZE])\n",
    "\n",
    "print(f\"Results for HotPotQA finetuning gpt-4o-mini with rejection sampling N={samples} and up to 1 attempts for each example with one model for all predictors. Tested on first {TEST_SIZE} of devset.\")\n",
    "print(f\"Non-finetuned model: {baseline_eval}\")\n",
    "print(f\"Non-finetuned bootstrapped model: {bs_eval}\")\n",
    "print(f\"Finetuned model: {bs_ft_eval}\")\n",
    "print(f\"Finetuned model with bootstrapping: {bs_ft_bs_eval}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
