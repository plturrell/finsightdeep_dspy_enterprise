{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "import dspy\n",
    "\n",
    "from dspy.datasets import HotPotQA\n",
    "import re\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "from dsp.utils import EM\n",
    "from dsp.utils.utils import deduplicate\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "mini = \"gpt-4o-mini-2024-07-18\"\n",
    "base_temp = 0.9\n",
    "\n",
    "lm = dspy.OpenAIModel(model=mini, max_tokens=500, temperature=base_temp)\n",
    "\n",
    "colbert_v2_endpoint = \"http://20.102.90.50:2017/wiki17_abstracts\"\n",
    "colbertv2 = dspy.ColBERTv2(url=colbert_v2_endpoint)\n",
    "\n",
    "dspy.settings.configure(rm=colbertv2, lm=lm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy.evaluate\n",
    "\n",
    "metric = dspy.evaluate.answer_exact_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and configure the datasets.\n",
    "TRAIN_SIZE = 500\n",
    "EVAL_SIZE = 500\n",
    "\n",
    "hotpot_dataset = HotPotQA(train_seed=1, eval_seed=2023, test_size=0, keep_details=\"type\")\n",
    "trainset = [x.with_inputs('question') for x in hotpot_dataset.train][:EVAL_SIZE]\n",
    "devset = [x.with_inputs('question') for x in hotpot_dataset.dev][:EVAL_SIZE]\n",
    "\n",
    "# Set up metrics\n",
    "NUM_THREADS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(num_threads=NUM_THREADS, display_progress=True)\n",
    "evaluate = Evaluate(devset=devset, metric=metric, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMH(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(2)]\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n",
    "\n",
    "    def forward(self, question, return_trace=False):\n",
    "        context = []\n",
    "        for hop in range(2):\n",
    "            search_query = self.generate_query[hop](context=context, question=question).search_query\n",
    "            passages = self.retrieve(search_query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        x = self.generate_answer(context=context, question=question).copy(context=context)\n",
    "        \n",
    "        if return_trace:\n",
    "            return x, dspy.settings.trace\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt.random_search import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "program_params = {\n",
    "    \"passages_per_hop\": 3,\n",
    "}\n",
    "\n",
    "COMPILE = False\n",
    "\n",
    "if COMPILE:\n",
    "    max_bootstrapped_demos, max_labeled_demos, num_candidate_programs = 3,3,6\n",
    "    config = dict(max_bootstrapped_demos=max_bootstrapped_demos, num_candidate_programs=num_candidate_programs, num_threads=NUM_THREADS)\n",
    "    teleprompter = BootstrapFewShotWithRandomSearch(metric=metric, **config)\n",
    "    basicmh_bs = teleprompter.compile(BasicMH(**program_params), trainset=trainset[:100], valset=devset[:150])\n",
    "    basicmh_bs.save(f\"basicmh_{max_bootstrapped_demos}_{max_labeled_demos}_{num_candidate_programs}.json\")\n",
    "\n",
    "    baseline_eval = evaluate(BasicMH(**program_params), devset=devset[:300])\n",
    "    bs_eval = evaluate(basicmh_bs, devset=devset[:300])\n",
    "else:\n",
    "    basicmh_bs = BasicMH(**program_params)\n",
    "    basicmh_bs.load(\"basicmh_3_3_6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 259 / 500  (51.8): 100%|██████████| 500/500 [00:01<00:00, 321.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset with length 777 to trainset_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 76 / 125  (60.8): 100%|██████████| 125/125 [00:00<00:00, 350.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset with length 228 to devset_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from dspy.teleprompt.finetune_teleprompter import bootstrap_data_multiple_rounds, DataCollectionCallback, build_prompt_completion_data_from_trace\n",
    "from typing import Callable\n",
    "import ujson\n",
    "\n",
    "samples = 500\n",
    "\n",
    "callback = DataCollectionCallback(num_correct=1, max_attempts=2)\n",
    "\n",
    "dspy.settings.configure(experimental=True)\n",
    "dc_kwargs = {\n",
    "    # \"exclude_demos\":True, \n",
    "    \"sampling_temperature_base\": base_temp,\n",
    "    \"sampling_temperature_delta\":0.0001,\n",
    "    \"next_round_dataset_callback\": callback.move_on_callback_correct_with_max,\n",
    "    \"num_threads\": NUM_THREADS,\n",
    "}\n",
    "\n",
    "dataset_filenames = {\"trainset_data.jsonl\": trainset[:samples], \"devset_data.jsonl\": devset[:int(samples/4)]}\n",
    "\n",
    "def write_data(data, filename):\n",
    "    # get the bootstrapped data for num_rounds=1, but using the callback\n",
    "    dataset = bootstrap_data_multiple_rounds(basicmh_bs, data, metric, num_rounds=1, **dc_kwargs)\n",
    "    # Flatmap the lists of prompt completions gives by each trace\n",
    "    dataset = sum([build_prompt_completion_data_from_trace(result[\"trace\"], program=basicmh_bs, exclude_demos=True) for result in dataset if result[\"score\"]], [])\n",
    "    # Format the data for finetuning using the LM\n",
    "    dataset = lm.format_data_for_vanilla_finetuning(dataset)\n",
    "    print(\"Writing dataset with length\", len(dataset), \"to\", filename)\n",
    "    with open(filename, \"w\") as f:\n",
    "        for line in dataset:\n",
    "            f.write(ujson.dumps(line) + \"\\n\")\n",
    "\n",
    "for key, data in dataset_filenames.items():\n",
    "    write_data(data, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "# lm.()\n",
    "# lm.get_finetune(\"trainset_data.jsonl\", \"devset_data.jsonl\", method=\"SFT\")\n",
    "future_lm = dspy.OpenAIModel.load_from_job_id(\"ftjob-tmccSJv170p4gNFJUYwXdCSq\")\n",
    "finetuned_lm = future_lm\n",
    "finetuned_lm.kwargs[\"temperature\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert finetuned_lm.kwargs[\"model\"] != lm.kwargs[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicmh_bs_ft = BasicMH(**program_params)\n",
    "basicmh_bs_ft._set_all_predictor_lms(finetuned_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 3 traces per predictor.\n",
      "Will attempt to bootstrap 6 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 106 / 250  (42.4): 100%|██████████| 250/250 [00:03<00:00, 71.56it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 42.4 for set: [0, 0, 0]\n",
      "New best score: 42.4 for seed -3\n",
      "Scores so far: [42.4]\n",
      "Best score: 42.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 134 / 250  (53.6): 100%|██████████| 250/250 [00:29<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 53.6 for set: [16, 16, 16]\n",
      "New best score: 53.6 for seed -2\n",
      "Scores so far: [42.4, 53.6]\n",
      "Best score: 53.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:26<07:02,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 89 / 162  (54.9):  64%|██████▍   | 161/250 [01:00<00:34,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 8  (50.0):   3%|▎         | 8/250 [10:14<5:10:00, 76.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 90 / 163  (55.2):  65%|██████▌   | 163/250 [01:00<00:26,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 142 / 250  (56.8): 100%|██████████| 250/250 [01:31<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 56.8 for set: [16, 16, 16]\n",
      "New best score: 56.8 for seed -1\n",
      "Scores so far: [42.4, 53.6, 56.8]\n",
      "Best score: 56.8\n",
      "Average of max per entry across top 1 scores: 0.568\n",
      "Average of max per entry across top 2 scores: 0.64\n",
      "Average of max per entry across top 3 scores: 0.656\n",
      "Average of max per entry across top 5 scores: 0.656\n",
      "Average of max per entry across top 8 scores: 0.656\n",
      "Average of max per entry across top 9999 scores: 0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:15<08:17,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 133 / 250  (53.2): 100%|██████████| 250/250 [01:49<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 53.2 for set: [16, 16, 16]\n",
      "Scores so far: [42.4, 53.6, 56.8, 53.2]\n",
      "Best score: 56.8\n",
      "Average of max per entry across top 1 scores: 0.568\n",
      "Average of max per entry across top 2 scores: 0.64\n",
      "Average of max per entry across top 3 scores: 0.664\n",
      "Average of max per entry across top 5 scores: 0.676\n",
      "Average of max per entry across top 8 scores: 0.676\n",
      "Average of max per entry across top 9999 scores: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:04<07:14,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 136 / 250  (54.4): 100%|██████████| 250/250 [01:49<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 54.4 for set: [16, 16, 16]\n",
      "Scores so far: [42.4, 53.6, 56.8, 53.2, 54.4]\n",
      "Best score: 56.8\n",
      "Average of max per entry across top 1 scores: 0.568\n",
      "Average of max per entry across top 2 scores: 0.624\n",
      "Average of max per entry across top 3 scores: 0.664\n",
      "Average of max per entry across top 5 scores: 0.684\n",
      "Average of max per entry across top 8 scores: 0.684\n",
      "Average of max per entry across top 9999 scores: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:06<05:32,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 135 / 250  (54.0): 100%|██████████| 250/250 [01:46<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 54.0 for set: [16, 16, 16]\n",
      "Scores so far: [42.4, 53.6, 56.8, 53.2, 54.4, 54.0]\n",
      "Best score: 56.8\n",
      "Average of max per entry across top 1 scores: 0.568\n",
      "Average of max per entry across top 2 scores: 0.624\n",
      "Average of max per entry across top 3 scores: 0.648\n",
      "Average of max per entry across top 5 scores: 0.68\n",
      "Average of max per entry across top 8 scores: 0.688\n",
      "Average of max per entry across top 9999 scores: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:10<08:25,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 125 / 250  (50.0): 100%|██████████| 250/250 [13:00<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 50.0 for set: [16, 16, 16]\n",
      "Scores so far: [42.4, 53.6, 56.8, 53.2, 54.4, 54.0, 50.0]\n",
      "Best score: 56.8\n",
      "Average of max per entry across top 1 scores: 0.568\n",
      "Average of max per entry across top 2 scores: 0.624\n",
      "Average of max per entry across top 3 scores: 0.648\n",
      "Average of max per entry across top 5 scores: 0.68\n",
      "Average of max per entry across top 8 scores: 0.688\n",
      "Average of max per entry across top 9999 scores: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [17:22<28:40:26, 1042.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 133 / 250  (53.2): 100%|██████████| 250/250 [01:35<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 53.2 for set: [16, 16, 16]\n",
      "Scores so far: [42.4, 53.6, 56.8, 53.2, 54.4, 54.0, 50.0, 53.2]\n",
      "Best score: 56.8\n",
      "Average of max per entry across top 1 scores: 0.568\n",
      "Average of max per entry across top 2 scores: 0.624\n",
      "Average of max per entry across top 3 scores: 0.648\n",
      "Average of max per entry across top 5 scores: 0.68\n",
      "Average of max per entry across top 8 scores: 0.688\n",
      "Average of max per entry across top 9999 scores: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:44<08:35,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 143 / 250  (57.2): 100%|██████████| 250/250 [01:47<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 57.2 for set: [16, 16, 16]\n",
      "New best score: 57.2 for seed 5\n",
      "Scores so far: [42.4, 53.6, 56.8, 53.2, 54.4, 54.0, 50.0, 53.2, 57.2]\n",
      "Best score: 57.2\n",
      "Average of max per entry across top 1 scores: 0.572\n",
      "Average of max per entry across top 2 scores: 0.64\n",
      "Average of max per entry across top 3 scores: 0.66\n",
      "Average of max per entry across top 5 scores: 0.688\n",
      "Average of max per entry across top 8 scores: 0.692\n",
      "Average of max per entry across top 9999 scores: 0.692\n",
      "9 candidate programs found.\n",
      "[('retrieve', <dspy.retrieve.retrieve.Retrieve object at 0x12e43f670>), ('generate_query[0]', Predict(StringSignature(context, question -> rationale, search_query\n",
      "    instructions='Given the fields `context`, `question`, produce the fields `search_query`.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the output fields}. We ...', '__dspy_field_type': 'output'})\n",
      "    search_query = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Search Query:', 'desc': '${search_query}'})\n",
      "))), ('generate_query[1]', Predict(StringSignature(context, question -> rationale, search_query\n",
      "    instructions='Given the fields `context`, `question`, produce the fields `search_query`.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the output fields}. We ...', '__dspy_field_type': 'output'})\n",
      "    search_query = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Search Query:', 'desc': '${search_query}'})\n",
      "))), ('generate_answer', Predict(StringSignature(context, question -> rationale, answer\n",
      "    instructions='Given the fields `context`, `question`, produce the fields `answer`.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the output fields}. We ...', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
      ")))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RECOMPILE_FT_MODEL = True\n",
    "\n",
    "if RECOMPILE_FT_MODEL:\n",
    "    max_bootstrapped_demos, max_labeled_demos, num_candidate_programs = 3,3,6\n",
    "    config = dict(max_bootstrapped_demos=max_bootstrapped_demos, num_candidate_programs=num_candidate_programs, num_threads=NUM_THREADS)\n",
    "    bsfsrs_teleprompter = BootstrapFewShotWithRandomSearch(metric=metric, **config)\n",
    "    basicmh_bs_ft_bs = bsfsrs_teleprompter.compile(student=basicmh_bs_ft, trainset=trainset[:100], valset=devset[:250])\n",
    "    basicmh_bs_ft_bs.save('mini_bs_ft_bs_hpqa_100.json')\n",
    "    basicmh_bs_ft_bs._set_all_predictor_lms(finetuned_lm)\n",
    "else:\n",
    "    basicmh_bs_ft_bs = BasicMH(**program_params)\n",
    "    basicmh_bs_ft_bs.load('mini_bs_ft_bs_hpqa_100.json')\n",
    "    basicmh_bs_ft_bs._set_all_predictor_lms(finetuned_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of dsp.modules.lm failed: Traceback (most recent call last):\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/dilara-dspy/dsp/modules/lm.py\", line 152, in <module>\n",
      "    class TrainableLM(LM, ABC):\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/dilara-dspy/dsp/modules/lm.py\", line 153, in TrainableLM\n",
      "    def get_finetune(self, train_path: str, val_path: Optional[str], method: FinetuningMethod, **kwargs) -> Future['TrainableLM']:\n",
      "NameError: name 'FinetuningMethod' is not defined\n",
      "]\n",
      "[autoreload of dsp.modules.openai failed: Traceback (most recent call last):\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/dilara-dspy/dsp/modules/openai.py\", line 6, in <module>\n",
      "    from dsp.modules.lm import TrainableLM, FinetuningMethod\n",
      "ImportError: cannot import name 'TrainableLM' from 'dsp.modules.lm' (/Users/isaac.miller/projects/dspy-finetuning/dilara-dspy/dsp/modules/lm.py)\n",
      "]\n",
      "[autoreload of dspy failed: Traceback (most recent call last):\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/Users/isaac.miller/projects/dspy-finetuning/dilara-dspy/dspy/__init__.py\", line 42, in <module>\n",
      "    OpenAIModel = dsp.TrainableOpenAI\n",
      "AttributeError: module 'dsp' has no attribute 'TrainableOpenAI'\n",
      "]\n",
      "Average Metric: 130 / 300  (43.3): 100%|██████████| 300/300 [00:24<00:00, 12.29it/s]  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute '_get_lm_info_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m TEST_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      2\u001b[0m baseline_eval \u001b[38;5;241m=\u001b[39m evaluate(BasicMH(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprogram_params), devset\u001b[38;5;241m=\u001b[39mdevset[:TEST_SIZE])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mbaseline_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_lm_info_str\u001b[49m()\n\u001b[1;32m      4\u001b[0m bs_eval \u001b[38;5;241m=\u001b[39m evaluate(basicmh_bs, devset\u001b[38;5;241m=\u001b[39mdevset[:TEST_SIZE])\n\u001b[1;32m      5\u001b[0m bs_eval\u001b[38;5;241m.\u001b[39m_get_lm_info_str()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute '_get_lm_info_str'"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 300\n",
    "baseline_eval = evaluate(BasicMH(**program_params), devset=devset[:TEST_SIZE])\n",
    "bs_eval = evaluate(basicmh_bs, devset=devset[:TEST_SIZE])\n",
    "bs_ft_eval = evaluate(basicmh_bs_ft, devset=devset[:TEST_SIZE])\n",
    "bs_ft_bs_eval = evaluate(basicmh_bs_ft_bs, devset=devset[:TEST_SIZE])\n",
    "\n",
    "print(f\"Results for HotPotQA finetuning gpt-4o-mini with rejection sampling N={samples} and up to 1 attempts for each example with one model for all predictors. Tested on first {TEST_SIZE} of devset.\")\n",
    "print(f\"Non-finetuned model: {baseline_eval}\")\n",
    "print(f\"Non-finetuned bootstrapped model: {bs_eval}\")\n",
    "print(f\"Finetuned model: {bs_ft_eval}\")\n",
    "print(f\"Finetuned model with bootstrapping: {bs_ft_bs_eval}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
