{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end DSPy Workflows Guide "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Setup and Data Gathering**: gather and preprocess a dataset for a finetuning a DSPy pipeline using LLaMa 3 70B\n",
    "- **Fine-tuning**: tune a smaller (LLama 3 8B) LLM (LoRA / full param) on the pipeline\n",
    "- **Serving**: serve the pipeline as a production application that can autoscale, etc.\n",
    "- **Evaluation**: apply batch offline inference with Ray data and VLLM to quickly evaluate the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node Set up:\n",
    "\n",
    "We will be running everything on a head node that uses 8xL4 GPUs. I find that they are usually available and suitable for this usecase. You can also use any more powerful node.\n",
    "\n",
    "To change to use L4 GPUs, click the \"1 active node\" in the top right corner, then for workspace node, click the pencil icon and navigate to the L4 tab and select the 8xL4 option. If you do not see L4 in the list of GPUs, they may not be available on your cloud. Choose another kind of GPU (This notebook has been tested on X, and Y as alternatives) (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(work): DSPy installation cell\n",
    "# TODO(decision): are these changes going to be merged into DSPy main\n",
    "\n",
    "# Either pull from pip or install locally if using a non-public version\n",
    "\n",
    "# !pip install -e dspy-d\n",
    "# !pip install -r dspy-d/requirements.txt\n",
    "# !pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import dsp\n",
    "import os\n",
    "\n",
    "# TODO: include cache in notebook\n",
    "cache_dir = \"/home/ray/default/dspy_cache\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    \n",
    "os.environ[\"DSP_CACHEDIR\"] = cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Here are the different options for setting the OPENAI_API_KEY.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# (1) If you are running this notebook locally, you can set it in your shell:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# export OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Ensure that the OPENAI_API_KEY is set before continuing\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI_BASE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDSP_CACHEDIR\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Here are the different options for setting the OPENAI_API_KEY.\n",
    "# (1) If you are running this notebook locally, you can set it in your shell:\n",
    "# export OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>\n",
    "\n",
    "# (2) If you are running this notebook in a Google Colab, you can set it in\n",
    "#     the secrets panel and import by uncommenting the following lines:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# (3) You can set it directly in the notebook by uncommenting the following\n",
    "#     line with the appropriate key. If you pick this option, make sure to\n",
    "#     remove your key before sharing your notebook or committing to a\n",
    "#     repository.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = <OPENAI_API_KEY>\n",
    "\n",
    "\n",
    "necessary_env_vars = [\n",
    "    # \"OPENAI_API_KEY\",\n",
    "    # \"API_BASE\",\n",
    "    \"DSP_CACHEDIR\",\n",
    "    \"HF_TOKEN\"\n",
    "]\n",
    "\n",
    "for var in necessary_env_vars:\n",
    "    assert os.environ[var], f\"{var} is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if not ray.is_initialized():\n",
    "    ray.init(runtime_env={\"env_vars\": os.environ, \"py_modules\": [dspy, dsp]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to download the models that we will use for this notebook.\n",
    "\n",
    "In this case, it is the Instruct version of the Meta Llama 3 70B and 8B models.\n",
    "\n",
    "We can use the public Anyscale S3 bucket to download the models. We will use the cluster storage as the download location. (TODO: Is cluster storage okay for this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://large-dl-models-mirror/models--meta-llama--Meta-Llama-3-70B-Instruct/main-safetensors/ /mnt/cluster_storage/meta-llama--Meta-Llama-3-70B-Instruct --recursive\n",
    "!aws s3 cp s3://large-dl-models-mirror/models--meta-llama--Meta-Llama-3-8B-Instruct/main-safetensors/ /mnt/cluster_storage/meta-llama--Meta-Llama-3-8B-Instruct --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of a random number generator in this notebook. We are creating a Random object here to ensure that our notebook is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rng = random.Random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "from dspy.evaluate import Evaluate\n",
    "from dsp.utils.utils import deduplicate\n",
    "\n",
    "\n",
    "# We are setting the experimental flag to True to make use of the fine-tuning\n",
    "# features that are still in development.\n",
    "dspy.settings.configure(experimental=True)\n",
    "\n",
    "# Define the program\n",
    "class BasicMH(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, num_hops=2):\n",
    "        super().__init__()\n",
    "        self.num_hops = num_hops\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(self.num_hops)]\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.num_hops):\n",
    "            search_query = self.generate_query[hop](context=context, question=question).search_query\n",
    "            passages = self.retrieve(search_query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        answer = self.generate_answer(context=context, question=question).copy(context=context)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets breakdown what BasicMH does\n",
    "\n",
    "TODO: add explanation of what each part does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "TRAIN_SIZE = 1000\n",
    "DEV_SIZE = 500\n",
    "dataset = HotPotQA(train_seed=1, eval_seed=2023, test_size=0, only_hard_examples=True)\n",
    "trainset = [x.with_inputs('question') for x in dataset.train][:TRAIN_SIZE]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev][:DEV_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the metric and evaluator\n",
    "NUM_THREADS = 12\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate = Evaluate(devset=devset, metric=metric, num_threads=NUM_THREADS, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO(optional): Discuss LLM as judge vs exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dspy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the retriever model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Note that this is not hosted on Anyscale on purpose to represent a real-world scenario where you arent hosting your DB on Anyscale\u001b[39;00m\n\u001b[1;32m      3\u001b[0m COLBERT_V2_ENDPOINT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://20.102.90.50:2017/wiki17_abstracts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mdspy\u001b[49m\u001b[38;5;241m.\u001b[39mColBERTv2(url\u001b[38;5;241m=\u001b[39mCOLBERT_V2_ENDPOINT)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dspy' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare the retriever model\n",
    "# Note that this is not hosted on Anyscale on purpose to represent a real-world scenario where you arent hosting your DB on Anyscale\n",
    "COLBERT_V2_ENDPOINT = \"http://20.102.90.50:2017/wiki17_abstracts\"\n",
    "retriever = dspy.ColBERTv2(url=COLBERT_V2_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering baseline performance\n",
    "\n",
    "run evaluate on a base pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 1024\n",
    "MODEL_PARAMETERS = {\n",
    "  \"max_tokens\": MAX_TOKENS,\n",
    "  \"temperature\": 0,\n",
    "}\n",
    "\n",
    "vanilla_program = BasicMH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from deploy_dspy.async_llm import \n",
    "def evaluate_ray(program, devset, model, tokenizer, dspy_context_kwargs, tensor_parallel_size, num_gpus):\n",
    "    # need to get number of GPUs somehow\n",
    "    concurrency = num_gpus\n",
    "    count = devset.count()\n",
    "    batch_size = ceil(count / concurrency)\n",
    "    print(devset.map_batches(DSPyActor, \n",
    "               batch_size=batch_size,\n",
    "               num_gpus=1,\n",
    "               concurrency=concurrency,\n",
    "               fn_constructor_kwargs={\"batch_size\": batch_size}\n",
    "               ).take_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first get a baseline with the 8B model\n",
    "from deploy_dspy.async_llm import AsyncLLMWrapper\n",
    "from transformers import AutoTokenizer\n",
    "# We first need to create an instance of the LLM in order for DSPy to interact with it\n",
    "# We will discuss it in detail later, but this is an opportunity to take advantage of Anyscale specific integrations with DSPy and use the new VLLMOfflineEngine class. Because we have 8 GPUs and each one can fit a model instance, we can use 8 instances of the 8B model to run inference in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_8b = dspy.VLLMOfflineEngine(model=\"meta-llama/Meta-Llama-3-8B\", async_mode=False, **MODEL_PARAMETERS)\n",
    "\n",
    "with dspy.context(lm=llama_8b, rm=retriever):\n",
    "  print(\"Evaluating the vanilla program on the devset using the model to be trained (llama 8B)...\")\n",
    "  vanilla_8b_base_eval = evaluate(vanilla_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will \n",
    "\n",
    "llm = AsyncLLMWrapper(\"/mnt/cluster_storage/hf/cache/Meta-Llama-3-70B-Instruct\",\n",
    "            max_pending_requests=512,\n",
    "            tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-70B-Instruct\"),\n",
    "            enforce_eager=True,\n",
    "            engine_use_ray=False,\n",
    "            worker_use_ray=False,\n",
    "            enable_prefix_caching=True,\n",
    "            tensor_parallel_size=8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the language model\n",
    "llama_70b = dspy.VLLMOfflineEngine(model=\"meta-llama/Meta-Llama-3-70B\", async_mode=False, **MODEL_PARAMETERS)\n",
    "\n",
    "with dspy.context(lm=llama_70b, rm=retriever):\n",
    "  print(\"Evaluating the vanilla program on the devset using llama 70B...\")\n",
    "  vanilla_70b_base_eval = evaluate(vanilla_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope to bring the 8B performance up to at least 70B level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the LLaMa 70B pipeline\n",
    "\n",
    "run BSFS and MIPROv2 on the pipeline with the playground model, choose whichever one gets the best performance\n",
    "\n",
    "TODO: add in MIPROv2 ( If desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization hyperparameters\n",
    "from dspy.teleprompt.random_search import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Define the hyperparameters for prompt optimization\n",
    "MAX_BOOTSTRAPPED_DEMOS = 3\n",
    "MAX_LABELED_DEMOS = 3\n",
    "NUM_CANDIDATE_PROGRAMS = 6\n",
    "OPTIMIZER_NUM_TRAIN = 100\n",
    "OPTIMIZER_NUM_VAL = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training and validation sets for the optimizer using the original\n",
    "# trainset. This ensures that our \"devset\" is left untouched.\n",
    "shuffled_trainset = [d for d in trainset]\n",
    "rng.shuffle(shuffled_trainset)\n",
    "optimizer_trainset = shuffled_trainset[:OPTIMIZER_NUM_TRAIN]\n",
    "optimizer_valset = shuffled_trainset[OPTIMIZER_NUM_TRAIN:OPTIMIZER_NUM_TRAIN+OPTIMIZER_NUM_VAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "bfrs_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=metric,\n",
    "    max_bootstrapped_demos=MAX_BOOTSTRAPPED_DEMOS,\n",
    "    max_labeled_demos=MAX_LABELED_DEMOS,\n",
    "    num_candidate_programs=NUM_CANDIDATE_PROGRAMS,\n",
    "    num_threads=NUM_THREADS\n",
    ")\n",
    "\n",
    "# Compile the optimizer and evaluate\n",
    "with dspy.context(lm=llama_70b, rm=retriever):\n",
    "    vanilla_program = BasicMH()\n",
    "    bfrs_base_program = bfrs_optimizer.compile(vanilla_program, trainset=optimizer_trainset, valset=optimizer_valset)\n",
    "    bfrs_base_eval = evaluate(bfrs_base_program, devset=devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a baseline for the finetuned model, we can run BSFS on the vanilla program with the smaller 8B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dspy.context(lm=llama_8b, rm=retriever):\n",
    "    vanilla_program = BasicMH()\n",
    "    bfrs_8b_program = bfrs_optimizer.compile(vanilla_program, trainset=optimizer_trainset, valset=optimizer_valset)\n",
    "    bfrs_8b_eval = evaluate(bfrs_8b_program, devset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Data\n",
    "\n",
    "\n",
    "In this section, we bootstrap data for fine-tuning. In the code block below, we are deciding which program should be used to collect the bootstraps. We are setting this to the prompt optimized program, but one could also set this to the vanilla program, though doing so would lead to lower quality bootstraps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_program = bfrs_base_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt.finetune_teleprompter import bootstrap_data\n",
    "\n",
    "\n",
    "# Bootstrap traces using the prompt optimized program\n",
    "# TODO: Change to using the version with multiple retries\n",
    "with dspy.context(lm=llama_70b, rm=retriever):\n",
    "    bootstrapped_data = bootstrap_data(program=bootstrap_program, dataset=trainset, metric=metric, num_threads=NUM_THREADS)\n",
    "\n",
    "# The bootstrap_data function returns a list of dictionaries, each dictionary\n",
    "# corresponding to an exmaple in the data, with the the following keys and\n",
    "# values:\n",
    "# - The `example` field corresponding to the example itself\n",
    "# - The `prediction` field corresponding to the prediction made by the program\n",
    "#   on the example.\n",
    "# - The `trace` field corresponding to the trace generated by the program on the\n",
    "#   example.\n",
    "# - The `score` field corresponding to the metric score of the example, if the\n",
    "#   metric is provided. Otherwise, it is not included in the data.\n",
    "print(f\"\\nTrainset has {len(trainset)} examples.\")\n",
    "print(f\"Bootstrapped data has {len(bootstrapped_data)} examples.\")\n",
    "print(f\"Bootstrapped data dictionaries have the following keys: {list(bootstrapped_data[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.shuffle(bootstrapped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the bootstrapped data to only include examples where the metric score\n",
    "# is 1\n",
    "filtered_bootstrapped_data = [d for d in bootstrapped_data if d['score']]\n",
    "print(f\"Filtered bootstrapped data has {len(filtered_bootstrapped_data)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt.finetune_teleprompter import convert_to_module_level_prompt_completion_data\n",
    "\n",
    "# To train a model on our program traces, we need a dataset that consists of\n",
    "# the prompts and the completions for each module in our program. We can use the\n",
    "# helper method `convert_to_prompt_completion_data` to generate these traces.\n",
    "# This function takes in a list of dictionaries, each of which must contain the\n",
    "# \"trace\" field, which is used for generating the prompts and completions for\n",
    "# each module in the program. The function returns a list of dictionaries, each\n",
    "# of which contains the keys \"prompt\" and \"completion\", among others. Refer to\n",
    "# the documentation for more information.\n",
    "#\n",
    "# The `exclude_demos` flag controls whether the demonstrations should be\n",
    "# included in the prompts. We are setting it to True here to have the \"vanilla\"\n",
    "# prompts as part of our finetuning data.\n",
    "finetune_data = convert_to_module_level_prompt_completion_data(filtered_bootstrapped_data, exclude_demos=True)\n",
    "print(f\"For each datapoint, we get one prompt completion pair for each module in our program of {len(vanilla_program.predictors())} modules.\")\n",
    "print(f\"As a result, finetune data has {len(filtered_bootstrapped_data)} * {len(bootstrap_program.predictors())} = {len(finetune_data)} examples.\")\n",
    "print(f\"The finetune data dictionaries have the following keys: {list(finetune_data[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at an example prompt completion pair!\n",
    "print(\"Example prompt:\")\n",
    "print(finetune_data[0]['prompt'])\n",
    "print(\"-\"*50)\n",
    "print(\"Example completion:\")\n",
    "print(finetune_data[0]['completion'])\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "Take the best pipeline from the previous step and run BSFT on it with the LLama 3 8B model thru LLM forge.\n",
    "\n",
    "NOTE: I think this is the part that needs the most work\n",
    "\n",
    "TODO: Outline what steps are still needed\n",
    "\n",
    "- Dev time estimate TBD based on outlining steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_llama_8b = dspy.TrainableAnyscaleLM(model=\"meta-llama/Meta-Llama-3-8B\", **MODEL_PARAMETERS)\n",
    "\n",
    "# Showing that the model is an instance of the TrainableLM class\n",
    "isinstance(student_llama_8b, dspy.TrainableLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "\n",
    "train_path = \"trainset_data.jsonl\"\n",
    "\n",
    "print(f\"Writing dataset with length {len(finetune_data)} to {train_path}\")\n",
    "with open(train_path, \"w\") as f:\n",
    "    ujson.dump(finetune_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.modules.lm import TrainingMethod\n",
    "\n",
    "USE_CACHED_MODEL = False\n",
    "\n",
    "if USE_CACHED_MODEL:\n",
    "  CACHED_FINETUNED_MODEL_ID = \"\"\n",
    "  # Support this case\n",
    "  finetuned_lm = dspy.TrainableAnyscaleLM(model=CACHED_FINETUNED_MODEL_ID, **MODEL_PARAMETERS)\n",
    "else:\n",
    "    # Define the hyperparameters for finetuning. To look at the specifics of\n",
    "    # what kinds of arguments should be passed to `get_finetune`, refer to the\n",
    "    # documentation for the particular subclass you are using, in this case,\n",
    "    # `dspy.TrainableAnyscaleLM`.\n",
    "    hyperparameters = {\n",
    "        \"n_epochs\": 3\n",
    "    }\n",
    "\n",
    "    # Select the training method. Must be one of the options in the\n",
    "    # `TrainingMethod` enum.\n",
    "    method = TrainingMethod.SFT\n",
    "\n",
    "    # Get a future object for the finetuned language model. This will get populated\n",
    "    # with the finetuned model once the training is complete.\n",
    "    # TODO: This is a big point of work\n",
    "    # Need to grab this from the other workspace\n",
    "    future_finetuned_lm = student_llama_8b.get_finetune(\n",
    "        method=method,\n",
    "        train_path=train_path,\n",
    "        hyperparameters=hyperparameters\n",
    "    )\n",
    "\n",
    "    # Call the result method on the future object to get the finetuned language.\n",
    "    # This will block the execution until the training is complete.\n",
    "    finetuned_lm = future_finetuned_lm.result()\n",
    "\n",
    "# Record the model ID to access the model again in the future. You can directly\n",
    "# substitute this model ID for the `CACHED_FINETUNED_MODEL_ID` variable above,\n",
    "# and the next line will load the model directly.\n",
    "finetuned_lm.kwargs['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Throughout this section, anything using 8B model (or technically 70B too) should use the new evaluate with ray data batch offline(or technically online) inference.\n",
    "\n",
    "Probably worth testing offline with 8x8 threads vs just 64 threads to see if it makes a meaningful difference.\n",
    "\n",
    "## Performance comparisons\n",
    "\n",
    "- 70B\n",
    "- 70B BSFS\n",
    "- 8B\n",
    "- 8B BSFT\n",
    "- 8B BSFT + BSFS\n",
    "\n",
    "---\n",
    "- Note: Allegedly really easy\n",
    "- Dev time estimate: 1 day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the vanilla program performs with the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dspy.context(lm=finetuned_lm, rm=retriever):\n",
    "    vanilla_program = BasicMH()\n",
    "    vanilla_finetuned_eval = evaluate(vanilla_program, devset=devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try optimizing the program with the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dspy.context(lm=finetuned_lm, rm=retriever):\n",
    "    vanilla_program = BasicMH()\n",
    "    bfrs_finetuned_program = bfrs_optimizer.compile(vanilla_program, trainset=optimizer_trainset, valset=optimizer_valset)\n",
    "    bfrs_finetuned_eval = evaluate(bfrs_finetuned_program, devset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now we can compare all iterations of this pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for HotPotQA fine-tuning  with a starting trainset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrainset\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Out of the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trainset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples, only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_bootstrapped_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples were judged as successful using a prompt optimized with BootstrapFewShotRandomSearch prompt optimized (bfrs) program on the base model. Traces from these examples are used for SFT. The reported results are computed on a held-out devset of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(devset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Base model (vanilla program): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvanilla_base_eval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Base model (bfrs program): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbfrs_base_eval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "# Now we can compare all iterations of this pipeline\n",
    "print(f\"Results for HotPotQA fine-tuning LLaMa 8Bwith a starting trainset with {len(trainset)}. Out of the {len(trainset)} examples, only {len(filtered_bootstrapped_data)} examples were judged as successful using a prompt optimized with BootstrapFewShotRandomSearch prompt optimized (bfrs) program on the base model. Traces from these examples are used for SFT. The reported results are computed on a held-out devset of {len(devset)} examples.\\n\")\n",
    "print(f\"    70B model (vanilla program): {vanilla_70b_base_eval}\")\n",
    "print(f\"    70B model (bfrs program): {bfrs_base_eval}\")\n",
    "print(f\"    8B model (vanilla program): {vanilla_8b_base_eval}\")\n",
    "print(f\"    8B model (bfrs program): {bfrs_8b_eval}\")\n",
    "print(f\"    8B model (finetuned program): {vanilla_finetuned_eval}\")\n",
    "print(f\"    8B model (finetuned bfrs program): {bfrs_finetuned_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the new offline batch inference to evaluate the finetuned model with optimized program on the entire devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement once done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving\n",
    "\n",
    "This is the second biggest unknown\n",
    "I imagine it to be easy, but crazier things have happened\n",
    "\n",
    "I need to keep a reference or link to the LLM forge job inside the LM.finetune method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_program = BasicMH()\n",
    "\n",
    "with dspy.context(lm=finetuned_lm, rm=retriever):\n",
    "  print(\"Evaluating the vanilla program on the devset using the finetuned model...\")\n",
    "  vanilla_finetune_eval = evaluate(vanilla_program, devset=devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch offline inference\n",
    "- Compare running inference using \n",
    "    - Ray Data \n",
    "    - multithreading on local VLLM thru HTTP\n",
    "    - Multithreading to Ray Serve instance thru HTTP\n",
    "- Dev time estimate: 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
