llmforge dev finetune model_config_dspy_5466321956575438383.yaml
[2024-09-13 11:02:33,257] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-09-13 11:02:35,838 INFO worker.py:1603 -- Connecting to existing Ray cluster at address: 10.0.0.51:6379...
2024-09-13 11:02:35,845 INFO worker.py:1779 -- Connected to Ray cluster. View the dashboard at https://session-fkvdirx4bzefi53sjl55m7asad.i.anyscaleuserdata.com 
2024-09-13 11:02:35,867 INFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_9cd9916dd96808308cf05dcd7146e3282b92d1c1.zip' (7.93MiB) to Ray cluster...
2024-09-13 11:02:35,900 INFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_9cd9916dd96808308cf05dcd7146e3282b92d1c1.zip'.
2024-09-13 11:02:35,976 INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-09-13_10-29-19_680506_6030/logs/ray-data
2024-09-13 11:02:35,976 INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ExpandPaths] -> TaskPoolMapOperator[ReadFiles]
✔️  Dataset execution finished in 1.66 seconds: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.65s/it]]
- ExpandPaths: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.65s/it]
- ReadFiles: 0 active, 0 queued, [cpu: 0.0, objects: 12.8KB]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.65s/it]
Downloading the tokenizer ...
Tokenizer init done.e, 0 queued, [cpu: 0.0, objects: 12.8KB]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.65s/it]
2024-09-13 11:02:41,658 INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-09-13_10-29-19_680506_6030/logs/ray-data
2024-09-13 11:02:41,658 INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ExpandPaths] -> TaskPoolMapOperator[ReadFiles]
✔️  Dataset execution finished in 0.17 seconds: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.07it/s]]
- ExpandPaths: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.03it/s]
- ReadFiles: 0 active, 0 queued, [cpu: 0.0, objects: 12.8KB]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.02it/s]
Statistics for ds_size:
- ReadFi162: 0 active, 0 queued, [cpu: 0.0, objects: 12.8KB]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.03it/s]

Statistics for message:
        max: 2.000
        min: 2.000
        median: 2.000
        mean: 2.000
        p95: 2.000
        p5: 2.000

Statistics for token:
        max: 1311.000
        min: 166.000
        median: 537.500
        mean: 510.117
        p95: 938.550
        p5: 172.000

Train dataset stats:
Statistics for ds_size:
        162

Statistics for message:
        max: 2.000
        min: 2.000
        median: 2.000
        mean: 2.000
        p95: 2.000
        p5: 2.000

Statistics for token:
        max: 1311.000
        min: 166.000
        median: 537.500
        mean: 510.117
        p95: 938.550
        p5: 172.000

2024-09-13 11:02:42,174 INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-09-13_10-29-19_680506_6030/logs/ray-data
2024-09-13 11:02:42,175 INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ExpandPaths] -> TaskPoolMapOperator[ReadFiles]
✔️  Dataset execution finished in 0.19 seconds: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.41it/s]]
- ExpandPaths: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.38it/s]
- ReadFiles: 0 active, 0 queued, [cpu: 0.0, objects: 12.8KB]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.38it/s]
Downloading the tokenizer ...
Tokenizer init done.e, 0 queued, [cpu: 0.0, objects: 12.8KB]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.38it/s]
2024-09-13 11:02:46,331 INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-09-13_10-29-19_680506_6030/logs/ray-data
2024-09-13 11:02:46,331 INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ExpandPaths] -> TaskPoolMapOperator[ReadFiles]
✔️  Dataset execution finished in 0.16 seconds: 100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.48it/s]]
- ExpandPaths: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.44it/s]
- ReadFiles: 0 active, 0 queued, [cpu: 0.0, objects: 12.8KB]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.43it/s]
Statistics for ds_size:
- ReadFi162: 0 active, 0 queued, [cpu: 0.0, objects: 12.8KB]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.44it/s]

Statistics for message:
        max: 2.000
        min: 2.000
        median: 2.000
        mean: 2.000
        p95: 2.000
        p5: 2.000

Statistics for token:
        max: 1311.000
        min: 166.000
        median: 537.500
        mean: 510.117
        p95: 938.550
        p5: 172.000

Valid dataset stats:
Statistics for ds_size:
        162

Statistics for message:
        max: 2.000
        min: 2.000
        median: 2.000
        mean: 2.000
        p95: 2.000
        p5: 2.000

Statistics for token:
        max: 1311.000
        min: 166.000
        median: 537.500
        mean: 510.117
        p95: 938.550
        p5: 172.000

Using train_batch_size_per_device = 16
Using eval_batch_size_per_device = 16
2024-09-13 11:02:48,551 INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-09-13_10-29-19_680506_6030/logs/ray-data
2024-09-13 11:02:48,552 INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ExpandPaths] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition]
✔️  Dataset execution finished in 0.21 seconds: 100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 39.12it/s]]
- ExpandPaths: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.85it/s]
- ReadFiles: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.85it/s]
- Repartition: 0 active, 0 queued, [cpu: 0.0, objects: 18.8KB], 0 output: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 38.76it/s]
  *- Split Repartition: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 38.73it/s]
2024-09-13 11:02:48,817 INFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-09-13_10-29-19_680506_6030/logs/ray-data
2024-09-13 11:02:48,819,INFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ExpandPaths] -> TaskPoolMapOperator[ReadFiles] -> AllToAllOperator[Repartition]
✔️  Dataset execution finished in 0.28 seconds: 100%|█████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 30.11it/s]]
- ExpandPaths: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.74it/s]
- ReadFiles: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.74it/s]
- Repartition: 0 active, 0 queued, [cpu: 0.0, objects: 18.8KB], 0 output: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 29.89it/s]
  *- Split Repartition: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 29.87it/s]

View detailed results here: storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3-8B-Instruct/TorchTrainer_2024-09-13_11-02-49
To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-09-13_10-29-19_680506_6030/artifacts/2024-09-13_11-02-50/TorchTrainer_2024-09-13_11-02-49/driver_artifacts`
(raylet) Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 1848, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1882, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 985, in ray._raylet.raise_if_dependency_failed
ray.exceptions.RaySystemError: System error: libcudnn.so.9: cannot open shared object file: No such file or directory
traceback: Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 423, in deserialize_objects
    obj = self._deserialize_object(data, metadata, object_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 280, in _deserialize_object
    return self._deserialize_msgpack_data(data, metadata_fields)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 235, in _deserialize_msgpack_data
    python_objects = self._deserialize_pickle5_data(pickle5_data)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 225, in _deserialize_pickle5_data
    obj = pickle.loads(in_band)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/llmforge/configs.py", line 26, in <module>
    from transformers.utils.generic import PaddingStrategy
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/__init__.py", line 34, in <module>
    from .generic import (
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py", line 462, in <module>
    import torch.utils._pytree as _torch_pytree
  File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 2293, in ray._raylet.task_execution_handler
  File "python/ray/_raylet.pyx", line 2189, in ray._raylet.execute_task_with_cancellation_handler
  File "python/ray/_raylet.pyx", line 1844, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1845, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 2083, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1076, in ray._raylet.store_task_errors
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/function_manager.py", line 603, in temporary_actor_method
    raise RuntimeError(
RuntimeError: The actor with name _Inner failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:

Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/function_manager.py", line 642, in _load_actor_class_from_gcs
    actor_class = pickle.loads(pickled_class)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/train/torch/__init__.py", line 3, in <module>
    import torch  # noqa: F401
    ^^^^^^^^^^^^
  File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory

An unexpected internal error occurred while the worker was executing a task.
(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffc2c07a9c70f6ba51475003f509000000 Worker ID: 64bc11b31120c2493f1f96931a730ede4217da0304ce79ae98c60611 Node ID: eb5271e2aff59bf6b0bc65f5bd1b0518329be52ac9b329e7e1618b0c Worker IP address: 10.0.0.51 Worker port: 10263 Worker PID: 44950 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 1848, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1882, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 985, in ray._raylet.raise_if_dependency_failed
ray.exceptions.RaySystemError: System error: libcudnn.so.9: cannot open shared object file: No such file or directory
traceback: Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 423, in deserialize_objects
    obj = self._deserialize_object(data, metadata, object_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 280, in _deserialize_object
    return self._deserialize_msgpack_data(data, metadata_fields)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 235, in _deserialize_msgpack_data
    python_objects = self._deserialize_pickle5_data(pickle5_data)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 225, in _deserialize_pickle5_data
    obj = pickle.loads(in_band)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/llmforge/configs.py", line 26, in <module>
    from transformers.utils.generic import PaddingStrategy
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/__init__.py", line 34, in <module>
    from .generic import (
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py", line 462, in <module>
    import torch.utils._pytree as _torch_pytree
  File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 2293, in ray._raylet.task_execution_handler
  File "python/ray/_raylet.pyx", line 2189, in ray._raylet.execute_task_with_cancellation_handler
  File "python/ray/_raylet.pyx", line 1844, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1845, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 2083, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1076, in ray._raylet.store_task_errors
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/function_manager.py", line 603, in temporary_actor_method
    raise RuntimeError(
RuntimeError: The actor with name _Inner failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:

Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/function_manager.py", line 642, in _load_actor_class_from_gcs
    actor_class = pickle.loads(pickled_class)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/train/torch/__init__.py", line 3, in <module>
    import torch  # noqa: F401
    ^^^^^^^^^^^^
  File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory

An unexpected internal error occurred while the worker was executing a task.(TemporaryActor pid=44950) libcudnn.so.9: cannot open shared object file: No such file or directory
(TemporaryActor pid=44950) Traceback (most recent call last):
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 423, in deserialize_objects
(TemporaryActor pid=44950)     obj = self._deserialize_object(data, metadata, object_ref)
(TemporaryActor pid=44950)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 280, in _deserialize_object
(TemporaryActor pid=44950)     return self._deserialize_msgpack_data(data, metadata_fields)
(TemporaryActor pid=44950)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 235, in _deserialize_msgpack_data
(TemporaryActor pid=44950)     python_objects = self._deserialize_pickle5_data(pickle5_data)

(TemporaryActor pid=44950)                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 225, in _deserialize_pickle5_data
(TemporaryActor pid=44950)     obj = pickle.loads(in_band)
(TemporaryActor pid=44950)           ^^^^^^^^^^^^^^^^^^^^^
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/llmforge/configs.py", line 26, in <module>
(TemporaryActor pid=44950)     from transformers.utils.generic import PaddingStrategy
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/__init__.py", line 26, in <module>
(TemporaryActor pid=44950)     from . import dependency_versions_check
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
(TemporaryActor pid=44950)     from .utils.versions import require_version, require_version_core
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/__init__.py", line 34, in <module>
(TemporaryActor pid=44950)     from .generic import (
(TemporaryActor pid=44950)   File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py", line 462, in <module>
(TemporaryActor pid=44950)     import torch.utils._pytree as _torch_pytree
(TemporaryActor pid=44950)   File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
(TemporaryActor pid=44950)     from torch._C import *  # noqa: F403
(TemporaryActor pid=44950)     ^^^^^^^^^^^^^^^^^^^^^^
(TemporaryActor pid=44950) ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory
2024-09-13 11:02:52,210 ERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_63c0f_00000
Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 2656, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py", line 873, in get_objects
    raise value
ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.
        class_name: with_parameters.<locals>._Inner
        actor_id: c2c07a9c70f6ba51475003f509000000
        pid: 44950
        namespace: a36ff62e-debb-4cc0-b3b8-80807b3d1084
        ip: 10.0.0.51
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 1848, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1882, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 985, in ray._raylet.raise_if_dependency_failed
ray.exceptions.RaySystemError: System error: libcudnn.so.9: cannot open shared object file: No such file or directory
traceback: Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 423, in deserialize_objects
    obj = self._deserialize_object(data, metadata, object_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 280, in _deserialize_object
    return self._deserialize_msgpack_data(data, metadata_fields)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 235, in _deserialize_msgpack_data
    python_objects = self._deserialize_pickle5_data(pickle5_data)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 225, in _deserialize_pickle5_data
    obj = pickle.loads(in_band)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/llmforge/configs.py", line 26, in <module>
    from transformers.utils.generic import PaddingStrategy
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/__init__.py", line 34, in <module>
    from .generic import (
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py", line 462, in <module>
    import torch.utils._pytree as _torch_pytree
  File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 2293, in ray._raylet.task_execution_handler
  File "python/ray/_raylet.pyx", line 2189, in ray._raylet.execute_task_with_cancellation_handler
  File "python/ray/_raylet.pyx", line 1844, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1845, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 2083, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1076, in ray._raylet.store_task_errors
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/function_manager.py", line 603, in temporary_actor_method
    raise RuntimeError(
RuntimeError: The actor with name _Inner failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:

Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/function_manager.py", line 642, in _load_actor_class_from_gcs
    actor_class = pickle.loads(pickled_class)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/train/torch/__init__.py", line 3, in <module>
    import torch  # noqa: F401
    ^^^^^^^^^^^^
  File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory

An unexpected internal error occurred while the worker was executing a task.

Training errored after 0 iterations at 2024-09-13 11:02:52. Total running time: 2s
Error file: /tmp/ray/session_2024-09-13_10-29-19_680506_6030/artifacts/2024-09-13_11-02-50/TorchTrainer_2024-09-13_11-02-49/driver_artifacts/TorchTrainer_63c0f_00000_0_2024-09-13_11-02-50/error.txt
2024-09-13 11:02:52,420 INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3-8B-Instruct/TorchTrainer_2024-09-13_11-02-49' in 0.2078s.

2024-09-13 11:02:52,421 ERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_63c0f_00000]
2024-09-13 11:02:52,541 WARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):
- TorchTrainer_63c0f_00000: FileNotFoundError('Could not fetch metrics for TorchTrainer_63c0f_00000: both result.json and progress.csv were not found at storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3-8B-Instruct/TorchTrainer_2024-09-13_11-02-49/TorchTrainer_63c0f_00000_0_2024-09-13_11-02-50')
ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.
        class_name: with_parameters.<locals>._Inner
        actor_id: c2c07a9c70f6ba51475003f509000000
        pid: 44950
        namespace: a36ff62e-debb-4cc0-b3b8-80807b3d1084
        ip: 10.0.0.51
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 1848, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1882, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 985, in ray._raylet.raise_if_dependency_failed
ray.exceptions.RaySystemError: System error: libcudnn.so.9: cannot open shared object file: No such file or directory
traceback: Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 423, in deserialize_objects
    obj = self._deserialize_object(data, metadata, object_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 280, in _deserialize_object
    return self._deserialize_msgpack_data(data, metadata_fields)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 235, in _deserialize_msgpack_data
    python_objects = self._deserialize_pickle5_data(pickle5_data)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/serialization.py", line 225, in _deserialize_pickle5_data
    obj = pickle.loads(in_band)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/llmforge/configs.py", line 26, in <module>
    from transformers.utils.generic import PaddingStrategy
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/__init__.py", line 34, in <module>
    from .generic import (
  File "/home/ray/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py", line 462, in <module>
    import torch.utils._pytree as _torch_pytree
  File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 2293, in ray._raylet.task_execution_handler
  File "python/ray/_raylet.pyx", line 2189, in ray._raylet.execute_task_with_cancellation_handler
  File "python/ray/_raylet.pyx", line 1844, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1845, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 2083, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 1076, in ray._raylet.store_task_errors
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/function_manager.py", line 603, in temporary_actor_method
    raise RuntimeError(
RuntimeError: The actor with name _Inner failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:

Traceback (most recent call last):
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/function_manager.py", line 642, in _load_actor_class_from_gcs
    actor_class = pickle.loads(pickled_class)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/train/torch/__init__.py", line 3, in <module>
    import torch  # noqa: F401
    ^^^^^^^^^^^^
  File "/tmp/ray/session_2024-09-13_10-29-19_680506_6030/runtime_resources/pip/336ad1b083212aa0bd1c3c9af4a6492437c5e2c5/virtualenv/lib/python3.11/site-packages/torch/__init__.py", line 290, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcudnn.so.9: cannot open shared object file: No such file or directory

An unexpected internal error occurred while the worker was executing a task.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ray/anaconda3/bin/llmforge", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/llmforge/cli.py", line 337, in finetune
    run_finetuning(ft_config, llmforge_callbacks=llmforge_callbacks)
  File "/home/ray/anaconda3/lib/python3.11/site-packages/llmforge/train.py", line 183, in run_finetuning
    results = trainer.fit()
              ^^^^^^^^^^^^^
  File "/home/ray/anaconda3/lib/python3.11/site-packages/ray/train/base_trainer.py", line 638, in fit
    raise TrainingFailedError(
ray.train.base_trainer.TrainingFailedError: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.
To continue this run, you can use: `trainer = TorchTrainer.restore("storage-bucket-cld-tffbxe9ia5phqr1unxhz4f7e1e/org_4snvy99zwbmh4gbtk64jfqggmj/cld_tffbxe9ia5phqr1unxhz4f7e1e/artifact_storage/isaac__miller/llmforge-finetuning/meta-llama/Meta-Llama-3-8B-Instruct/TorchTrainer_2024-09-13_11-02-49")`.
To start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.