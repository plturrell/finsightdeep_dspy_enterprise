---
sidebar_position: 5
---

# dspy.HFClientVLLM

### Usage

```python
lm = dspy.HFClientVLLM(model="meta-llama/Llama-2-7b-hf", port=8080, url="http://localhost")
```

### Prerequisites

Refer to the [vLLM Server](https://github.com/stanfordnlp/dspy/blob/local_models_docs/docs/using_local_models.md#vllm-server) section of the `Using Local Models` documentation.

### Constructor

Refer to [`dspy.TGI`](#tgi) documentation. Replace with `HFClientVLLM`.

### Methods

Refer to [`dspy.OpenAI`](#openai) documentation.